{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Racer**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JE8PtTjUZ48h"
      },
      "source": [
        "##### **Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cGA7gxZHZ4Nf"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from typing import Tuple, Union\n",
        "import pandas as pd\n",
        "#import torch\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "sys.path.append(\"..\")\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import pandas as pd\n",
        "import time\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fErj3uRiZ9xZ"
      },
      "source": [
        "##### **Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-XPP8OYMaCwG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(CVXPY) Mar 27 05:04:28 PM: Encountered unexpected exception importing solver GLOP:\n",
            "RuntimeError('Unrecognized new version of ortools (9.6.2534). Expected < 9.5.0.Please open a feature request on cvxpy to enable support for this version.')\n",
            "(CVXPY) Mar 27 05:04:28 PM: Encountered unexpected exception importing solver PDLP:\n",
            "RuntimeError('Unrecognized new version of ortools (9.6.2534). Expected < 9.5.0.Please open a feature request on cvxpy to enable support for this version.')\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from optbinning import MulticlassOptimalBinning as MOB, MDLP\n",
        "\n",
        "\n",
        "class RACERPreprocessor:\n",
        "    def __init__(self, target: str=\"auto\", max_n_bins=32, max_num_splits=32):\n",
        "        \"\"\"RACER preprocessing step that quantizes numerical columns and dummy encodes the categorical ones.\n",
        "        Quantization is based on the optimal binning algorithm for \"multiclass\" tasks and the entropy-based MDLP\n",
        "        algorithm for \"binary\" tasks.\n",
        "\n",
        "        Args:\n",
        "            target (str, optional): Whether the task is \"multiclass\" or \"binary\" classification. Defaults to \"auto\" which attempts automatically infer the task from `y`.\n",
        "            max_n_bins (int, optional): Maximum number of bins to quantize in. Defaults to 32.\n",
        "            max_num_splits (int, optional): Maximum number of splits to consider at each partition for MDLP. Defaults to 32.\n",
        "        \"\"\"\n",
        "        assert target in [\"multiclass\", \"binary\", \"auto\"], \"`target` must either be 'multiclass', 'binary' or 'auto'.\"\n",
        "        if target == \"multiclass\":\n",
        "            self._quantizer = MOB(max_n_bins=max_n_bins)\n",
        "        elif target == \"binary\":\n",
        "            self._quantizer = MDLP(max_candidates=max_num_splits)\n",
        "        else:\n",
        "            self._quantizer = \"infer\"\n",
        "            self._max_n_bins = max_n_bins\n",
        "            self._max_candidates = max_num_splits\n",
        "\n",
        "    def fit_transform(\n",
        "        self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]\n",
        "    ) -> Tuple[Union[pd.DataFrame, np.ndarray], Union[pd.DataFrame, np.ndarray]]:\n",
        "        \"\"\"Preprocesses the dataset by replacing nominal vaues with dummy variables.\n",
        "        Converts to numpy boolean arrays and returns the dataset. All numerical values are discretized\n",
        "        using an optimal binning strategy that employs a decision tree as a preprocessing step.\n",
        "\n",
        "        Args:\n",
        "            X (Union[pd.DataFrame, np.ndarray]): Features vector\n",
        "            y (Union[pd.DataFrame, np.ndarray]): Targets vector\n",
        "\n",
        "        Returns:\n",
        "            Tuple[Union[pd.DataFrame, np.ndarray], Union[pd.DataFrame, np.ndarray]]: Transformed features and targets vectors.\n",
        "        \"\"\"\n",
        "        X, y = pd.DataFrame(X), pd.DataFrame(y)\n",
        "        if self._quantizer == \"infer\":\n",
        "            uniques = y.nunique().values\n",
        "            if uniques > 2:\n",
        "                self._quantizer = MOB(max_candidates=self._max_num_splits)\n",
        "            else:\n",
        "                self._quantizer = MDLP(max_n_bins=self._max_n_bins)\n",
        "        numerics_X = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        if numerics_X:\n",
        "            for col in numerics_X:\n",
        "                self._quantizer.fit(X[col].values, np.squeeze(y.values))\n",
        "                bins = [X[col].min()] + self._quantizer.splits.tolist() + [X[col].max()]\n",
        "                X[col] = pd.cut(X[col], bins=bins, include_lowest=True, labels=False)\n",
        "        X, y = X.astype(\"category\"), y.astype(\"category\")\n",
        "        X = pd.get_dummies(X).to_numpy()\n",
        "        y = pd.get_dummies(y).to_numpy()\n",
        "        return X, y\n",
        "\n",
        "    def fit(\n",
        "        self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]\n",
        "    ):\n",
        "        raise NotImplementedError(\n",
        "            \"Applying transformation across different datasets is not currently supported. Please use fit_transform instead.\"\n",
        "        )\n",
        "\n",
        "    def transform(self):\n",
        "        raise NotImplementedError(\n",
        "            \"Applying transformation across different datasets is not currently supported. Please use fit_transform instead.\"\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hGYU6SABaFjW"
      },
      "source": [
        "##### **RACER_numpy**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5ha0tey00jcA"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "from numpy import (\n",
        "    bitwise_and as AND,\n",
        "    bitwise_not as NOT,\n",
        "    bitwise_or as OR,\n",
        "    bitwise_xor as XOR,\n",
        ")\n",
        "\n",
        "\n",
        "def XNOR(input: np.ndarray, other: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Computes the XNOR gate. (semantically the same as `input == other`)\n",
        "\n",
        "    Args:\n",
        "        input (np.ndarray): Input array\n",
        "        other (np.ndarray): Other input array\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: XNOR(input, other) as an array\n",
        "    \"\"\"\n",
        "    return NOT(XOR(input, other))\n",
        "\n",
        "\n",
        "class RACER:\n",
        "    def __init__(\n",
        "        self,\n",
        "        alpha=0.9,\n",
        "        suppress_warnings=False,\n",
        "        benchmark=False,\n",
        "    ):\n",
        "        \"\"\"Initialize the RACER class\n",
        "\n",
        "        Args:\n",
        "            alpha (float, optional): Value of alpha according to the RACER paper. Defaults to 0.9.\n",
        "            suppress_warnings (bool, optional): Whether to suppress any warnings raised during prediction. Defaults to False.\n",
        "            benchmark (bool, optional): Whether to time the `fit` method for benchmark purposes. Defaults to False.\n",
        "        \"\"\"\n",
        "        self._alpha, self._beta = alpha, 1.0 - alpha\n",
        "        self._suppress_warnings = suppress_warnings\n",
        "        self._benchmark = benchmark\n",
        "        self._has_fit = False\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\"Fits the RACER algorithm on top of input data X and targets y.\n",
        "        The code is written in close correlation to the pseudo-code provided in the RACER paper with some slight modifications.\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Features vector\n",
        "            y (np.ndarray): Targets vector\n",
        "        \"\"\"\n",
        "        if self._benchmark:\n",
        "            from time import perf_counter\n",
        "\n",
        "            tic = perf_counter()\n",
        "\n",
        "        self._X, self._y = X, y\n",
        "        self._cardinality, self._rule_len = self._X.shape\n",
        "        self._classes = np.unique(self._y, axis=0)\n",
        "        self._class_indices = {\n",
        "            self._label_to_int(cls): np.where(XNOR(self._y, cls).min(axis=-1))[0]\n",
        "            for cls in self._classes\n",
        "        }\n",
        "\n",
        "        self._create_init_rules()\n",
        "\n",
        "        for cls in self._class_indices.keys():\n",
        "            for i in range(len(self._class_indices[cls])):\n",
        "                for j in range(i + 1, len(self._class_indices[cls])):\n",
        "                    self._process_rules(\n",
        "                        self._class_indices[cls][i], self._class_indices[cls][j]\n",
        "                    )\n",
        "\n",
        "        independent_indices = NOT(self._extants_covered)\n",
        "        self._extants_if, self._extants_then, self._fitnesses = (\n",
        "            self._extants_if[independent_indices],\n",
        "            self._extants_then[independent_indices],\n",
        "            self._fitnesses[independent_indices],\n",
        "        )\n",
        "\n",
        "        self._generalize_extants()\n",
        "\n",
        "        # https://stackoverflow.com/questions/64238462/numpy-descending-stable-arg-sort-of-arrays-of-any-dtype\n",
        "        args = (\n",
        "            len(self._fitnesses)\n",
        "            - 1\n",
        "            - np.argsort(self._fitnesses[::-1], kind=\"stable\")[::-1]\n",
        "        )\n",
        "\n",
        "        self._final_rules_if, self._final_rules_then, self._fitnesses = (\n",
        "            self._extants_if[args],\n",
        "            self._extants_then[args],\n",
        "            self._fitnesses[args],\n",
        "        )\n",
        "\n",
        "        self._finalize_rules()\n",
        "\n",
        "        self._has_fit = True\n",
        "\n",
        "        if self._benchmark:\n",
        "            self._bench_time = perf_counter() - tic\n",
        "\n",
        "    def predict(self, X: np.ndarray, convert_dummies=True) -> np.ndarray:\n",
        "        \"\"\"Given input X, predict label using RACER\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input features vector\n",
        "            convert_dummies (bool, optional): Whether to convert dummy labels back to integert format. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Label as predicted by RACER\n",
        "        \"\"\"\n",
        "        assert self._has_fit, \"RACER has not been fit yet.\"\n",
        "        labels = np.zeros((len(X), self._final_rules_then.shape[1]), dtype=bool)\n",
        "        found = np.zeros(len(X), dtype=bool)\n",
        "        for i in range(len(self._final_rules_if)):\n",
        "            covered = self._covered(X, self._final_rules_if[i])\n",
        "            labels[AND(covered, NOT(found))] = self._final_rules_then[i]\n",
        "            found[covered] = True\n",
        "            all_found = found.sum() == len(X)\n",
        "            if all_found:\n",
        "                break\n",
        "\n",
        "        if not all_found:\n",
        "            # if not self._suppress_warnings:\n",
        "            #     print(\n",
        "            #         f\"WARNING: RACER was unable to find a perfect match for {len(X) - found.sum()} instances out of {len(X)}\"\n",
        "            #     )\n",
        "            #     print(\n",
        "            #         \"These instances will be labelled as the majority class during training.\"\n",
        "            #     )\n",
        "            leftover_indices = np.where(NOT(found))[0]\n",
        "            for idx in leftover_indices:\n",
        "                labels[idx] = self._closest_match(X[idx])\n",
        "\n",
        "        if convert_dummies:\n",
        "            labels = np.argmax(labels, axis=-1)\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _bool2str(self, bool_arr: np.ndarray) -> str:\n",
        "        \"\"\"Converts a boolean array to a human-readable string\n",
        "\n",
        "        Args:\n",
        "            bool_arr (np.ndarray): The input boolean array\n",
        "\n",
        "        Returns:\n",
        "            str: Human-readable string output\n",
        "        \"\"\"\n",
        "        return np.array2string(bool_arr.astype(int), separator=\"\")\n",
        "\n",
        "    def display_rules(self) -> None:\n",
        "        \"\"\"Print out the final rules\"\"\"\n",
        "        assert self._has_fit, \"RACER has not been fit yet.\"\n",
        "        print(\"Algorithm Parameters:\")\n",
        "        print(f\"\\t- Alpha: {self._alpha}\")\n",
        "        if self._benchmark:\n",
        "            print(f\"\\t- Time to fit: {self._bench_time}s\")\n",
        "        print(\n",
        "            f\"\\nFinal Rules ({len(self._final_rules_if)} total): (if --> then (label) | fitness)\"\n",
        "        )\n",
        "        for i in range(len(self._final_rules_if)):\n",
        "            print(\n",
        "                f\"\\t{self._bool2str(self._final_rules_if[i])} -->\"\n",
        "                f\" {self._bool2str(self._final_rules_then[i])}\"\n",
        "                f\" ({self._label_to_int(self._final_rules_then[i])})\"\n",
        "                f\" | {self._fitnesses[i]}\"\n",
        "            )\n",
        "\n",
        "    def _closest_match(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Find the closest matching rule to `X` (This will be extended later)\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Input rule `X`\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Matched rule\n",
        "        \"\"\"\n",
        "        return self._majority_then\n",
        "\n",
        "    def score(self, X_test: np.ndarray, y_test: np.ndarray) -> float:\n",
        "        \"\"\"Returns accuracy on the provided test data.\n",
        "\n",
        "        Args:\n",
        "            X_test (np.ndarray): Test features vector\n",
        "            y_test (np.ndarray): Test targets vector\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy score\n",
        "        \"\"\"\n",
        "        assert self._has_fit, \"RACER has not been fit yet.\"\n",
        "        try:\n",
        "            from sklearn.metrics import accuracy_score\n",
        "        except ImportError as e:\n",
        "            raise ImportError(\n",
        "                \"scikit-learn is required to use the score function. Install wit `pip install scikit-learn`.\"\n",
        "            )\n",
        "        if y_test.ndim != 1 and y_test.shape[1] != 1:\n",
        "            y_test = np.argmax(y_test, axis=-1)\n",
        "        y_pred = self.predict(X_test)\n",
        "        return accuracy_score(y_test, y_pred)\n",
        "\n",
        "    def _fitness_fn(self, rule_if: np.ndarray, rule_then: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns fitness for a given rule according to the RACER paper\n",
        "\n",
        "        Args:\n",
        "            rule_if (np.ndarray): If part of a rule (x)\n",
        "            rule_then (np.ndarray): Then part of a rule (y)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Fitness score for the rule as defined in the RACER paper\n",
        "        \"\"\"\n",
        "        n_covered, n_correct = self._confusion(rule_if, rule_then)\n",
        "        accuracy = n_correct / n_covered\n",
        "        coverage = n_covered / self._cardinality\n",
        "        return self._alpha * accuracy + self._beta * coverage\n",
        "\n",
        "    def _covered(self, X: np.ndarray, rule_if: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns indices of instances if `X` that are covered by `rule_if`.\n",
        "        Note that rule covers instance if EITHER of the following holds in a bitwise manner:\n",
        "        1. instance[i] == 0\n",
        "        2. instance[i] == 1 AND rule[i] == 1\n",
        "\n",
        "        Args:\n",
        "            X (np.ndarray): Instances\n",
        "            rule_if (np.ndarray): If part of rule (x)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: An array containing indices in `X` that are covered by `rule_if`\n",
        "        \"\"\"\n",
        "        covered = OR(NOT(X), AND(rule_if, X)).min(axis=-1)\n",
        "        return covered\n",
        "\n",
        "    def _confusion(\n",
        "        self, rule_if: np.ndarray, rule_then: np.ndarray\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Returns n_correct and n_covered for instances classified by a rule.\n",
        "\n",
        "        Args:\n",
        "            rule_if (np.ndarray): If part of rule (x)\n",
        "            rule_then (np.ndarray): Then part of rule (y)\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, np.ndarray]: (n_covered, n_correct)\n",
        "        \"\"\"\n",
        "        covered = self._covered(self._X, rule_if)\n",
        "        n_covered = covered.sum()\n",
        "        y_covered = self._y[covered]\n",
        "        n_correct = XNOR(y_covered, rule_then).min(axis=-1).sum()\n",
        "        return n_covered, n_correct\n",
        "\n",
        "    def _get_majority(self) -> np.ndarray:\n",
        "        \"\"\"Return the majority rule_then from self._y\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Majority rule_then\n",
        "        \"\"\"\n",
        "        u, indices = np.unique(self._y, axis=0, return_inverse=True)\n",
        "        return u[np.bincount(indices).argmax()]\n",
        "\n",
        "    def _create_init_rules(self) -> None:\n",
        "        \"\"\"Creates an initial set of rules from theinput feature vectors\"\"\"\n",
        "        self._extants_if = self._X.copy()\n",
        "        self._extants_then = self._y.copy()\n",
        "        self._extants_covered = np.zeros(len(self._X), dtype=bool)\n",
        "        self._majority_then = self._get_majority()\n",
        "        self._fitnesses = np.array(\n",
        "            [\n",
        "                self._fitness_fn(rule_if, rule_then)\n",
        "                for rule_if, rule_then in zip(self._X, self._y)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def _composable(self, idx1: int, idx2: int) -> bool:\n",
        "        \"\"\"Returns true if two rules indicated by their indices are composable\n",
        "\n",
        "        Args:\n",
        "            idx1 (int): Index of the first rule\n",
        "            idx2 (int): Index of the second rule\n",
        "\n",
        "        Returns:\n",
        "            bool: True if labels match and neither of the rules are covered. False otherwise.\n",
        "        \"\"\"\n",
        "        labels_match = XNOR(self._extants_then[idx1], self._extants_then[idx2]).min()\n",
        "        return (\n",
        "            labels_match\n",
        "            and not self._extants_covered[idx1]\n",
        "            and not self._extants_covered[idx2]\n",
        "        )\n",
        "\n",
        "    def _process_rules(self, idx1: int, idx2: int) -> None:\n",
        "        \"\"\"Process two rules indiciated by their indices\n",
        "\n",
        "        Args:\n",
        "            idx1 (int): Index of the first rule\n",
        "            idx2 (int): Index of the second rule\n",
        "        \"\"\"\n",
        "        if self._composable(idx1, idx2):\n",
        "            composition = self._compose(self._extants_if[idx1], self._extants_if[idx2])\n",
        "            composition_fitness = self._fitness_fn(\n",
        "                composition, self._extants_then[idx1]\n",
        "            )\n",
        "            if composition_fitness > np.maximum(\n",
        "                self._fitnesses[idx1], self._fitnesses[idx2]\n",
        "            ):\n",
        "                self._update_extants(\n",
        "                    idx1, composition, self._extants_then[idx1], composition_fitness\n",
        "                )\n",
        "\n",
        "    def _compose(self, rule1: np.ndarray, rule2: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Composes rule1 with rule2\n",
        "\n",
        "        Args:\n",
        "            rule1 (np.ndarray): The first rule\n",
        "            rule2 (np.ndarray): The second rule\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The composed rule which is simply the bitwise OR of the two rules\n",
        "        \"\"\"\n",
        "        return OR(rule1, rule2)\n",
        "\n",
        "    def _update_extants(\n",
        "        self,\n",
        "        index: int,\n",
        "        new_rule_if: np.ndarray,\n",
        "        new_rule_then: np.ndarray,\n",
        "        new_rule_fitness: np.ndarray,\n",
        "    ):\n",
        "        \"\"\"Remove all rules from current extants that are covered by `new_rule`.\n",
        "        Then append new rule to extants.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index of `new_rule`\n",
        "            new_rule_if (np.ndarray): If part of `new_rule` (x)\n",
        "            new_rule_then (np.ndarray): Then part of `new_rule` (y)\n",
        "            new_rule_fitness (np.ndarray): Fitness of the `new_rule`\n",
        "        \"\"\"\n",
        "        same_class_indices = self._class_indices[self._label_to_int(new_rule_then)]\n",
        "        covered = self._covered(self._extants_if[same_class_indices], new_rule_if)\n",
        "        self._extants_covered[same_class_indices[covered]] = True\n",
        "        self._extants_covered[index] = False\n",
        "        self._extants_if[index], self._extants_then[index], self._fitnesses[index] = (\n",
        "            new_rule_if,\n",
        "            new_rule_then,\n",
        "            new_rule_fitness,\n",
        "        )\n",
        "\n",
        "    def _label_to_int(self, label: np.ndarray) -> int:\n",
        "        \"\"\"Converts dummy label to int\n",
        "\n",
        "        Args:\n",
        "            label (np.ndarray): Label to convert\n",
        "\n",
        "        Returns:\n",
        "            int: Converted label\n",
        "        \"\"\"\n",
        "        return int(np.argmax(label))\n",
        "\n",
        "    def _generalize_extants(self) -> None:\n",
        "        \"\"\"Generalize the extants by flipping every 0 to a 1 and checking if the fitness improves.\"\"\"\n",
        "        new_extants_if = np.zeros_like(self._extants_if, dtype=bool)\n",
        "        for i in range(len(self._extants_if)):\n",
        "            for j in range(len(self._extants_if[i])):\n",
        "                if not self._extants_if[i][j]:\n",
        "                    self._extants_if[i][j] = True\n",
        "                    fitness = self._fitness_fn(\n",
        "                        self._extants_if[i], self._extants_then[i]\n",
        "                    )\n",
        "                    if fitness > self._fitnesses[i]:\n",
        "                        self._fitnesses[i] = fitness\n",
        "                    else:\n",
        "                        self._extants_if[i][j] = False\n",
        "            new_extants_if[i] = self._extants_if[i]\n",
        "        self._extants_if = new_extants_if\n",
        "\n",
        "    def _finalize_rules(self) -> None:\n",
        "        \"\"\"Removes redundant rules to form the final ruleset\"\"\"\n",
        "        temp_rules_if = self._final_rules_if\n",
        "        temp_rules_then = self._final_rules_then\n",
        "        temp_rules_fitnesses = self._fitnesses\n",
        "        i = 0\n",
        "        while i < len(temp_rules_if) - 1:\n",
        "            mask = np.ones(len(temp_rules_if), dtype=bool)\n",
        "            covered = self._covered(temp_rules_if[i + 1 :], temp_rules_if[i])\n",
        "            mask[i + 1 :][covered] = False\n",
        "            temp_rules_if, temp_rules_then, temp_rules_fitnesses = (\n",
        "                temp_rules_if[mask],\n",
        "                temp_rules_then[mask],\n",
        "                temp_rules_fitnesses[mask],\n",
        "            )\n",
        "            i += 1\n",
        "\n",
        "        self._final_rules_if, self._final_rules_then, self._fitnesses = (\n",
        "            temp_rules_if,\n",
        "            temp_rules_then,\n",
        "            temp_rules_fitnesses,\n",
        "        )\n",
        "        \n",
        "# *********************************************************************************************************************************\n",
        "    \n",
        "    def raceknn(self, X: np.ndarray, X_train: np.ndarray, Y_train: np.ndarray, kmeasure: int, convert_dummies=True) -> np.ndarray:\n",
        "        \"\"\"Given input X, predict label using RACER\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): input features vector\n",
        "        X_train (np.ndarray): training features vector\n",
        "        Y_train (np.ndarray): training labels vector\n",
        "        kmeasure (int): the minimum number of samples required to assign a label using majority voting\n",
        "        convert_dummies (bool): whether to convert the output to a one-dimensional array\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: label as predicted by RACER\n",
        "    \"\"\"\n",
        "        assert self._has_fit, \"RACER has not been fit yet.\"\n",
        "        labelss = np.zeros((len(X), self._final_rules_then.shape[1]), dtype=bool)\n",
        "        founds = np.zeros(len(X), dtype=bool)\n",
        "        xy__train = [None] * len(X)\n",
        "        for i in range(len(self._final_rules_if)):\n",
        "            covereds = self._covered(X, self._final_rules_if[i])\n",
        "            xtrain_cover = self._covered(X_train, self._final_rules_if[i])\n",
        "            if i < len(X) :\n",
        "                bes_rulei = self._closest_match_if(X[i])\n",
        "            for indi,vali in enumerate(bes_rulei) :\n",
        "                if xtrain_cover.sum() < kmeasure :\n",
        "                    covi = self._covered(X_train, bes_rulei[indi])\n",
        "                    xtrain_cover = OR(xtrain_cover, covi)\n",
        "                        \n",
        "            labelss[AND(covereds, NOT(founds))] = self._final_rules_then[i]\n",
        "            founds[covereds] = True\n",
        "            for index,val in enumerate(covereds):\n",
        "                if val:\n",
        "                    xy__train[index] = xtrain_cover\n",
        "            if founds.sum() == len(X):  # -> every instance was matched to a rule\n",
        "                break\n",
        "\n",
        "        all_found = founds.sum() == len(X)\n",
        "        if not all_found:\n",
        "            # print(\n",
        "            #     f\"Warning: RACER was unable to find a perfect match for {len(X) - founds.sum()} instances out of {len(X)}.\"\n",
        "            # )\n",
        "            # print(\n",
        "            #     \"Labels for these instances will be determined by a closest match algorithm.\"\n",
        "            # )\n",
        "            leftover_indices = np.where(~founds)[0]\n",
        "            #print(leftover_indices)\n",
        "            for idx in leftover_indices:\n",
        "                labelss[idx] = self._closest_match(X[idx])\n",
        "                bes_rule = self._closest_match_if(X[idx])\n",
        "                xtrain_closest = self._covered(X_train, bes_rule[0])\n",
        "                for indl,vall in enumerate(bes_rule) :\n",
        "                    if xtrain_closest.sum() < kmeasure :\n",
        "                        covl = self._covered(X_train, bes_rule[indl])\n",
        "                        xtrain_closest = OR(xtrain_closest, covl)\n",
        "                        \n",
        "                    xy__train[idx] = xtrain_closest\n",
        "                    \n",
        "        if convert_dummies:\n",
        "            labelss = np.argmax(labelss, axis=-1)\n",
        "\n",
        "        return xy__train\n",
        "\n",
        "    def _closest_match_if(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Find the closest matching rule to `X`\n",
        "        Args:\n",
        "            X (np.ndarray): input `X`\n",
        "        Returns:\n",
        "            np.ndarray: matched rule\n",
        "        \"\"\"\n",
        "        # coverage := count of covered bits by a rule. Higher is better.\n",
        "        gum = 0.6\n",
        "        int_X = X.astype(int)  # <- cast boolean array to integer array\n",
        "        overlap = OR(NOT(int_X), AND(self._final_rules_if, int_X)).sum(axis=-1)\n",
        "        overlap = overlap / self._rule_len  # -> normalize by rule length\n",
        "        scores = np.multiply(gum * overlap, (1 - gum) * self._fitnesses)\n",
        "        scores = np.argsort(-scores)\n",
        "        final_best_rules = self._final_rules_if[scores]\n",
        "        return final_best_rules\n",
        "    \n",
        "    def _closest_match_then(self, X: np.ndarray) -> np.ndarray:\n",
        "            \"\"\"Find the closest matching rule to `X`\n",
        "            Args:\n",
        "                X (np.ndarray): input `X`\n",
        "            Returns:\n",
        "                np.ndarray: matched rule\n",
        "            \"\"\"\n",
        "            # coverage := count of covered bits by a rule. Higher is better.\n",
        "            gum = 0.6\n",
        "            int_X = X.astype(int)  # <- cast boolean array to integer array\n",
        "            overlap = OR(NOT(int_X), AND(self._final_rules_if, int_X)).sum(axis=-1)\n",
        "            overlap = overlap / self._rule_len  # -> normalize by rule length\n",
        "            scores = np.multiply(gum * overlap, (1 - gum) * self._fitnesses)\n",
        "            scores = np.argsort(-scores)\n",
        "            final_best_then = self._final_rules_then[scores]\n",
        "            return final_best_then"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IpzwmUvpaPhp"
      },
      "source": [
        "### **Dataset &  Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set initial values\n",
        "measure_k = 3\n",
        "Alpha = 0.9"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Load Dataset and Split that**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 307 entries, 0 to 306\n",
            "Data columns (total 36 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   class   307 non-null    object\n",
            " 1   1       307 non-null    int64 \n",
            " 2   2       307 non-null    int64 \n",
            " 3   3       307 non-null    int64 \n",
            " 4   4       307 non-null    int64 \n",
            " 5   5       307 non-null    int64 \n",
            " 6   6       307 non-null    int64 \n",
            " 7   7       307 non-null    int64 \n",
            " 8   8       307 non-null    int64 \n",
            " 9   9       307 non-null    int64 \n",
            " 10  10      307 non-null    int64 \n",
            " 11  11      307 non-null    int64 \n",
            " 12  12      307 non-null    int64 \n",
            " 13  13      307 non-null    int64 \n",
            " 14  14      307 non-null    int64 \n",
            " 15  15      307 non-null    int64 \n",
            " 16  16      307 non-null    int64 \n",
            " 17  17      307 non-null    int64 \n",
            " 18  18      307 non-null    int64 \n",
            " 19  19      307 non-null    int64 \n",
            " 20  20      307 non-null    int64 \n",
            " 21  21      307 non-null    int64 \n",
            " 22  22      307 non-null    int64 \n",
            " 23  23      307 non-null    int64 \n",
            " 24  24      307 non-null    int64 \n",
            " 25  25      307 non-null    int64 \n",
            " 26  26      307 non-null    int64 \n",
            " 27  27      307 non-null    int64 \n",
            " 28  28      307 non-null    int64 \n",
            " 29  29      307 non-null    int64 \n",
            " 30  30      307 non-null    int64 \n",
            " 31  31      307 non-null    int64 \n",
            " 32  32      307 non-null    int64 \n",
            " 33  33      307 non-null    int64 \n",
            " 34  34      307 non-null    int64 \n",
            " 35  35      307 non-null    int64 \n",
            "dtypes: int64(35), object(1)\n",
            "memory usage: 86.5+ KB\n",
            "Dataset information:\n",
            " None\n",
            "\n",
            "Dataset description:\n",
            "                 1           2           3           4           5           6   \n",
            "count  307.000000  307.000000  307.000000  307.000000  307.000000  307.000000  \\\n",
            "mean     3.514658    0.426710    1.485342    1.081433    0.045603    1.807818   \n",
            "std      1.728920    0.545633    0.849229    0.693019    0.558246    1.005965   \n",
            "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
            "25%      2.000000    0.000000    1.000000    1.000000    0.000000    1.000000   \n",
            "50%      4.000000    0.000000    2.000000    1.000000    0.000000    2.000000   \n",
            "75%      5.000000    1.000000    2.000000    2.000000    0.000000    3.000000   \n",
            "max      6.000000    1.000000    2.000000    2.000000    1.000000    3.000000   \n",
            "\n",
            "                7           8           9          10  ...          26   \n",
            "count  307.000000  307.000000  307.000000  307.000000  ...  307.000000  \\\n",
            "mean     1.625407    0.540717    0.342020    0.778502  ...    0.094463   \n",
            "std      1.087544    0.836740    0.790234    0.991694  ...    0.472307   \n",
            "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
            "25%      1.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
            "50%      2.000000    1.000000    0.000000    1.000000  ...    0.000000   \n",
            "75%      3.000000    1.000000    1.000000    2.000000  ...    0.000000   \n",
            "max      3.000000    2.000000    2.000000    2.000000  ...    2.000000   \n",
            "\n",
            "               27          28          29          30          31          32   \n",
            "count  307.000000  307.000000  307.000000  307.000000  307.000000  307.000000  \\\n",
            "mean    -0.003257    0.423453    0.846906    0.071661    0.006515   -0.022801   \n",
            "std      0.261948    1.017737    1.624772    0.506247    0.442759    0.453167   \n",
            "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
            "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
            "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
            "75%      0.000000    1.000000    2.000000    0.000000    0.000000    0.000000   \n",
            "max      1.000000    3.000000    4.000000    1.000000    1.000000    1.000000   \n",
            "\n",
            "               33          34          35  \n",
            "count  307.000000  307.000000  307.000000  \n",
            "mean    -0.006515   -0.061889    0.130293  \n",
            "std      0.427743    0.403514    0.452980  \n",
            "min     -1.000000   -1.000000   -1.000000  \n",
            "25%      0.000000    0.000000    0.000000  \n",
            "50%      0.000000    0.000000    0.000000  \n",
            "75%      0.000000    0.000000    0.000000  \n",
            "max      1.000000    1.000000    2.000000  \n",
            "\n",
            "[8 rows x 35 columns]\n",
            "\n",
            "The X datset:\n",
            "    1  2  3  4  5  6  7  8  9 10  ... 26 27 28 29 30 31 32 33 34 35\n",
            "0  6  0  2  1  0  1  1  1  0  0  ...  0  0  0  4  0  0  0  0  0  0\n",
            "1  4  0  2  1  0  2  0  2  1  1  ...  0  0  0  4  0  0  0  0  0  0\n",
            "2  3  0  2  1  0  1  0  2  1  2  ...  0  0  0  4  0  0  0  0  0  0\n",
            "3  3  0  2  1  0  1  0  2  0  1  ...  0  0  0  4  0  0  0  0  0  0\n",
            "4  6  0  2  1  0  2  0  1  0  2  ...  0  0  0  4  0  0  0  0  0  0\n",
            "\n",
            "[5 rows x 35 columns] \n",
            "\n",
            "The Y dataset:\n",
            "                    class\n",
            "0  diaporthe-stem-canker\n",
            "1  diaporthe-stem-canker\n",
            "2  diaporthe-stem-canker\n",
            "3  diaporthe-stem-canker\n",
            "4  diaporthe-stem-canker\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\n",
        "    \"datasets/backup-large.data\",\n",
        "    names=[\n",
        "        \"class\",\n",
        "        \"1\",\n",
        "        \"2\",\n",
        "        \"3\",\n",
        "        \"4\",\n",
        "        \"5\",\n",
        "        \"6\",\n",
        "        \"7\",\n",
        "        \"8\",\n",
        "        \"9\",\n",
        "        \"10\",\n",
        "        \"11\",\n",
        "        \"12\",\n",
        "        \"13\",\n",
        "        \"14\",\n",
        "        \"15\",\n",
        "        \"16\",\n",
        "        \"17\",\n",
        "        \"18\",\n",
        "        \"19\",\n",
        "        \"20\",\n",
        "        \"21\",\n",
        "        \"22\",\n",
        "        \"23\",\n",
        "        \"24\",\n",
        "        \"25\",\n",
        "        \"26\",\n",
        "        \"27\",\n",
        "        \"28\",\n",
        "        \"29\",\n",
        "        \"30\",\n",
        "        \"31\",\n",
        "        \"32\",\n",
        "        \"33\",\n",
        "        \"34\",\n",
        "        \"35\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Dataset information:\\n\", df.info())\n",
        "\n",
        "print(\"\\nDataset description:\\n\", df.describe())\n",
        "\n",
        "\n",
        "# Spiliting X & Y in dataset\n",
        "X = df.drop(columns=['class']).astype('category')\n",
        "Y = df[['class']].astype('category')\n",
        "print(\"\\nThe X datset:\\n\", X.head(), \"\\n\\nThe Y dataset:\\n\", Y.head())\n",
        "\n",
        "# Fitting RACERPreprocessor() on dataset\n",
        "X, Y = RACERPreprocessor(target=\"multiclass\").fit_transform(X, Y)\n",
        "\n",
        "# Defining the KFold\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **RACEkNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "K measure is: 3 \tAlpha measure is: 0.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average execution time across all folds (EKNN): 0.20254864692687988\n",
            "\n",
            "Average accuracy across all folds (EKNN): 0.9023796932839767\n"
          ]
        }
      ],
      "source": [
        "KNNETimePerFold = []\n",
        "FinalAccuracyKNNEPerfold = []\n",
        "\n",
        "# Edited KNN\n",
        "KNNE = KNeighborsClassifier(n_neighbors=measure_k)\n",
        "\n",
        "print(\"K measure is:\", measure_k, \"\\tAlpha measure is:\", Alpha)\n",
        "\n",
        "# Initialize Racer with alpha measure\n",
        "racer = RACER(alpha=Alpha, suppress_warnings=False, benchmark=True)\n",
        "\n",
        "# Define lists to store execution time and accuracy per fold\n",
        "RacerTimePerFold = []\n",
        "FinalAccuracyRacerPerFold = []\n",
        "\n",
        "# Perform cross-validation\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "    \n",
        "    RacerST = time.time()  # Start timer\n",
        "    racer.fit(X_train, Y_train)\n",
        "    prediction = racer.score(X_test, Y_test)  # Predict for a single test data point\n",
        "    RacerET = time.time()  # End timer\n",
        "    \n",
        "    xy = racer.raceknn(X_test, X_train, Y_train, measure_k, True)\n",
        "    \n",
        "    x__train = []\n",
        "    y__train = []\n",
        "    for i,v in enumerate(xy):\n",
        "        x__train.append(X_train[v]) \n",
        "        y__train.append(Y_train[v])\n",
        "        \n",
        "    KNNEESDifference = []\n",
        "    PredictionsKNNEPerFold = []  # Clear predictions list for each fold\n",
        "    \n",
        "    for i in range(len(X_test)):\n",
        "        KNNEST = time.time()  # Start timer\n",
        "        KNNE.fit(x__train[i], y__train[i])\n",
        "        predictionE = KNNE.predict([X_test[i]])  # Predict for a single test data point\n",
        "        KNNEET = time.time()  # End timer\n",
        "        \n",
        "        PredictionsKNNEPerFold.append(predictionE[0])  # Append the predicted label (assumes single label prediction)\n",
        "        KNNEESDifference.append(KNNEET - KNNEST)  # Calculate elapsed time\n",
        "    \n",
        "    KNNETimePerFold.append(sum(KNNEESDifference)) \n",
        "    FinalAccuracyKNNEPerfold.append(accuracy_score(Y_test, PredictionsKNNEPerFold))  # Compute accuracy for each fold\n",
        "\n",
        "print(\"\\nAverage execution time across all folds (EKNN):\", np.mean(KNNETimePerFold))\n",
        "print(\"\\nAverage accuracy across all folds (EKNN):\", np.mean(FinalAccuracyKNNEPerfold))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
